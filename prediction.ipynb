{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Approach\n",
    "\n",
    "In this challenge, I started to use some idea from text generation to train my system. Because, text generation is based on previous elements and it generates the most probable next word. (Actually, it is depend on the _Temperature_ value, however, if you set this value like 0.4, it generate mostly most probable one.)\n",
    "\n",
    "There are different implementations for text generation, however, most of them is based on char-rnn. So that, we can not use these versions for word level based prediction. If we can use char based generation, [this implemantation](https://github.com/minimaxir/textgenrnn) will be one of the best option. Because, it is well optimized for memory usage and speed.\n",
    "\n",
    "_One last thing that textgenrnn can do that most char-rnn implementations can’t is generate a word level model (thanks to Keras’s tokenizers), where the model uses the n previous words/punctuation to predict the next word/punctuation._ [Source](https://minimaxir.com/2018/05/text-neural-networks/)\n",
    "\n",
    "So that, I will choose [this blogpost](https://medium.com/@david.campion/text-generation-using-bidirectional-lstm-and-doc2vec-models-1-3-8979eb65cb3a) as my intuition. I will change and tweak the methods according to my own need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATASET**\n",
    "\n",
    "I have tried [the blog authorship corpus.](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm) This corpus includes .xml files for blogpost. I thought that it can represent real human conservation.\n",
    "\n",
    "Firstly, I need to convert to .txt files. Also, when I check the dataset, I realize that, if the writer is younger than 21, it contains too much slang and typo. So that, I will use blogpost whose writers are older than 21. \n",
    "\n",
    "For xml manipulation, BeatifulSoup is one of the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import os\n",
    "\n",
    "xml_root_dir = glob.glob(os.path.join('./blogs/', '*xml'))\n",
    "\n",
    "for xml_file in xml_root_dir:\n",
    "    \n",
    "    if (int((xml_file.split('/')[-1]).split('.')[2]) > 21):\n",
    "    \n",
    "        try:\n",
    "            with open(xml_file) as fp:\n",
    "                soup = BeautifulSoup(fp, \"lxml\")\n",
    "\n",
    "                text_file = open('./blogs_txt/' + (xml_file.split('/')[-1])[:-4] + \".txt\", 'w')\n",
    "                posts = soup.findAll('post')\n",
    "                for single_post in posts:\n",
    "                    text_file.write(single_post.text)\n",
    "\n",
    "                text_file.close()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I train the LSTM system with this dataset, results are not satisfied. It mostly generates same words. I use simple LSTM model. Maybe, if we can clear more and train with more deep architecture, we can get good results. \n",
    "\n",
    "_Ps. I have trained it for 30 minutes. I don't supply the training result. Because I want to keep clear this notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I have tried [Cornell Movie Dialog Corpus.](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) We need to preprocess this txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cornell_movie_quotes_corpus/moviequotes.scripts.txt', encoding = \"ISO-8859-1\") as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip() for x in content]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cornell_clean.txt', 'w') as f:\n",
    "    counter = 0\n",
    "    for single_content in content:\n",
    "        single_sentence = (single_content.split('+++$+++')[-1])\n",
    "        f.write(single_sentence)\n",
    "        counter += 1\n",
    "        if (counter == 5000): # How many file will be taken\n",
    "            break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, GRU\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D\n",
    "from keras.models import load_model\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'save' # To store trained DL model\n",
    "file_list = glob.glob(os.path.join('./', 'cornell_clean.txt'))\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "sequences_step = 1 # Step to create sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use [Spacy](https://spacy.io) to tokenize my dataset. If we use _Keras_ tokenizer, we can not work on word-level creation or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    \"\"\"Append all words in one .txt files into one list.\n",
    "    \n",
    "    Arguments:\n",
    "    doc: The document's directory\"\"\"\n",
    "    \n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0', 'urlLink'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a list which inclued all words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "\n",
    "for input_file in file_list:\n",
    "    \n",
    "    # Read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    pattern = re.compile('[^a-zA-Z\\d\\s]')\n",
    "    data_revise = pattern.sub('', data)\n",
    "        \n",
    "    # Create sentences with spacy tokenizer.\n",
    "    doc = nlp(data)\n",
    "    wl = create_wordlist(doc)\n",
    "    wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionary, each specific word has own specific index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  6050\n"
     ]
    }
   ],
   "source": [
    "# Count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "# Size of vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "# Save the words and vocabulary as a pickle file\n",
    "with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 68710\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We need to create two different list. One list include the previous words, another list inclued the next word.\"\"\"\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s1.gifyu.com/images/try-2018-04-15-03.42.07.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This gif can give a idea to how system predict the next word. This gif was created for my music generation project.\"\"\"\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://s1.gifyu.com/images/try-2018-04-15-03.42.07.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We can not use this type of array directly. So that, we have to modify this data to use with LSTM. We need \n",
    "to convert into one-hot vector type array. \n",
    "\n",
    "List which includes previous words should have a dimension as number of sequences, number of words in sequences,\n",
    "number of words in vocabulary. The other list should have a dimension as number of sequences, \n",
    "number of words in vocabulary.\"\"\"\n",
    "\n",
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\", return_sequences=True, \n",
    "                                 kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal'),\n",
    "                            input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\",\n",
    "                                kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal')))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "model built!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 15, 64)            1557248   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6050)              393250    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6050)              0         \n",
      "=================================================================\n",
      "Total params: 1,975,330\n",
      "Trainable params: 1,975,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 32 # size of RNN\n",
    "learning_rate = 0.001 \n",
    "\n",
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61839 samples, validate on 6871 samples\n",
      "Epoch 1/2\n",
      "61839/61839 [==============================] - 494s 8ms/step - loss: 6.2289 - categorical_accuracy: 0.0711 - val_loss: 5.7886 - val_categorical_accuracy: 0.0688\n",
      "Epoch 2/2\n",
      "61839/61839 [==============================] - 542s 9ms/step - loss: 5.9084 - categorical_accuracy: 0.0771 - val_loss: 5.7305 - val_categorical_accuracy: 0.0956\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # minibatch size\n",
    "num_epochs = 2 # number of epochs\n",
    "\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "md.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./save/\"\n",
    "\n",
    "# Load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "# Load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \"a a a a a a a a a a a a i will need\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iniatate sentence\n",
    "seed_sentences = \"i will need\"\n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a i will need .   .     .   ?    \n"
     ]
    }
   ],
   "source": [
    "words_number = 10\n",
    "# Generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.2)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it can not create reasonable result with simple DNN model. Also, I have to clean the dataset with better methods to get better result. So that, I have tried to n-grams model. They works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## N-GRAMS\n",
    "\n",
    "[Idea](https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf)\n",
    "\n",
    "[Some Code](https://github.com/rikenshah/predict-next-word)\n",
    "\n",
    "\n",
    "I want to create the system which has low memory usage and need low CPU power. So that, I will try [python containers](https://docs.python.org/3/library/collections.html) and compare them.\n",
    "\n",
    "Firstly, I will start with how we can use txt file to create bi-grams. I will use Cornell Movie Quote Dataset as my text generation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "\"\"\"I just want to alpha-numeric characters.\"\"\"\n",
    "\n",
    "pattern = re.compile('[^a-zA-Z\\d\\s]')\n",
    "data_revise = pattern.sub('', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_revise = data_revise.lower()\n",
    "text = data_revise.split()\n",
    "nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "filtered = [w for w in text if nonPunct.match(w)]\n",
    "unigramsCounter = Counter(filtered)\n",
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "bigrams = ngrams(data_revise.split(), 2)\n",
    "\n",
    "bigramsCounter = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "tokens = nltk.word_tokenize(data_revise)\n",
    "\n",
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(tokens)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(bgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference between these 2 methods in terms of memory usage and CPU. I will use Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "# We need to save Counter as a pickle to use later.\n",
    "pickleDumps = \"pickleDumps/\"\n",
    "conditionalProbabilityFile = pickleDumps+\"conditionalProbabilityDictBigram.p\"\n",
    "bigramsCounterPath = pickleDumps+\"bigramsCounter.p\"\n",
    "\n",
    "\n",
    "conditionalProbabilityDict = OrderedDict() # Key: Bigram, Value: Prob. of bigram\n",
    "\n",
    "for words, count in bigramsCounter.items():\n",
    "    firstWord = words[0]\n",
    "    secondWord = words[1]\n",
    "    count = count\n",
    "    cProb = count * 1.0 / unigramsCounter[firstWord]\n",
    "    conditionalProbabilityDict[firstWord+\" \"+secondWord] = cProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(bigramsCounterPath,\"wb\")\n",
    "pickle.dump(bigramsCounter,file)\n",
    "file = open(conditionalProbabilityFile,\"wb\")\n",
    "pickle.dump(conditionalProbabilityDict,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can not enter input via Jupyter notebook. So we need to my python script. However, I will give some example from system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : i g\n",
    "i got : 0.016574585635359115\n",
    "\n",
    "i guess : 0.011740331491712707\n",
    "\n",
    "i get : 0.011740331491712707\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : you c\n",
    "you can : 0.0210688591983556\n",
    "\n",
    "you cant : 0.012846865364850977\n",
    "\n",
    "you could : 0.008735868448098663\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : for\n",
    "for the : 0.13903743315508021\n",
    "\n",
    "for a : 0.11497326203208556\n",
    "\n",
    "for you : 0.09358288770053476\n",
    "\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : for in\n",
    "for information : 0.00267379679144385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "As you can see, results are not so satisfied. I want to improve it. So that, I will use directly [COCA Corpus](https://www.ngrams.info/samples_coca1.asp) which contains the 1,000,000 most frequent 2, 3, 4, and 5-grams. I will use non-case sensitive corpus. _(To download it, you need to register with your e-mail. So, I cannot share wget download script to this Corpus)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./w2_.txt', encoding = \"ISO-8859-1\") as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip() for x in content]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import pickle, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "corpusPath = \"./\"\n",
    "fileName = \"w2_.txt\"\n",
    "pickleDumps = \"pickleDumps/\"\n",
    "conditionalProbabilityFile = pickleDumps+\"conditionalProbabilityDictBigram.p\"\n",
    "bigramsCounterPath = pickleDumps + \"bigramsCounter.p\"\n",
    "\n",
    "with open(corpusPath+fileName, encoding = \"ISO-8859-1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "bigramsCounter = {} # To store bigrams count. Key: single word, value: how many\n",
    "unigramsCounter = {} # To store unigrams count. Key: word pair, value: how many\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    # Removing \\n and \\r that were due to readline and splitting by tab\n",
    "    singleLine = line.replace('\\r','').replace('\\n','').split('\\t')\n",
    "    \n",
    "    bigramsCounter[singleLine[1], singleLine[2]] = int(singleLine[0])\n",
    "    \n",
    "    # If key does not exist, we need to create key, value pair with inital values\n",
    "    # If key exists then add the count of that unigram\n",
    "    if singleLine[1] in unigramsCounter.keys():\n",
    "        unigramsCounter[singleLine[1]] += int(singleLine[0])\n",
    "    else:\n",
    "        unigramsCounter[singleLine[1]] = int(singleLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import pickle\n",
    "\n",
    "conditionalProbabilityDict = OrderedDict() # key:bigram , value:probability\n",
    "for words, count in bigramsCounter.items():\n",
    "    firstWord = words[0]\n",
    "    secondWord = words[1]\n",
    "    count = count\n",
    "    cProb = count * 1.0 / unigramsCounter[firstWord]\n",
    "    conditionalProbabilityDict[firstWord+\" \"+secondWord] = cProb\n",
    "\n",
    "# print conditionalProbabilityDict\n",
    "file = open(conditionalProbabilityFile,\"wb\")\n",
    "pickle.dump(conditionalProbabilityDict,file)\n",
    "\n",
    "file = open(bigramsCounterPath,\"wb\")\n",
    "pickle.dump(bigramsCounter,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Usage for Pickle Files**\n",
    "\n",
    "If we use orderedDict to store bigrams, pickleDumps folder takes 83.8 MB memory. \n",
    "\n",
    "If use Counter to store bigrams, pickleDumps folder takes 71.3 MB memory.\n",
    "\n",
    "**Speed**\n",
    "\n",
    "I have tried both store method with different prediction string, there is no sufficient difference between them. However, I need to test with at least 10000 different scripts to understand difference.\n",
    "\n",
    "Mostly, it can give the result in 100ms-150ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a word to predict its next probable words (':)' for stopping) : for\n",
    "for the : 0.18615500817070693\n",
    "\n",
    "for a : 0.09376909228897273\n",
    "\n",
    "for example : 0.025409373405987343\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : for in\n",
    "for instance : 0.008036617330354772\n",
    "\n",
    "for information : 0.0013408959057072335\n",
    "\n",
    "for in : 0.0010947629860785409\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : can we g\n",
    "we got : 0.012135281736282812\n",
    "\n",
    "we get : 0.010222156277533184\n",
    "\n",
    "we go : 0.008274891311626734\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve result, I want to implement trigrams. However, because of limited time, I can not create combination of both method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-GRAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import pickle, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "corpusPath = \"./\"\n",
    "fileName = \"w3_.txt\"\n",
    "pickleDumps = \"pickleDumps/\"\n",
    "conditionalProbabilityFile = pickleDumps+\"conditionalProbabilityDictTrigram.p\"\n",
    "trigramsCounterPath = pickleDumps + \"trigramsCounter.p\"\n",
    "\n",
    "with open(corpusPath+fileName, encoding = \"ISO-8859-1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "trigramsCounter = {} # To store bigrams count. Key: single word, value: how many\n",
    "unigramsCounter = {} # To store unigrams count. Key: word pair, value: how many\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    # Removing \\n and \\r that were due to readline and splitting by tab\n",
    "    singleLine = line.replace('\\r','').replace('\\n','').split('\\t')\n",
    "    \n",
    "    trigramsCounter[singleLine[1], singleLine[2], singleLine[3]] = int(singleLine[0])\n",
    "    \n",
    "    # If key does not exist, we need to create key, value pair with inital values\n",
    "    # If key exists then add the count of that unigram\n",
    "    if singleLine[1] in unigramsCounter.keys():\n",
    "        unigramsCounter[singleLine[1]] += int(singleLine[0])\n",
    "    else:\n",
    "        unigramsCounter[singleLine[1]] = int(singleLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import pickle\n",
    "\n",
    "conditionalProbabilityDict = OrderedDict() # key:bigram , value:probability\n",
    "for words, count in trigramsCounter.items():\n",
    "    firstWord = words[0]\n",
    "    secondWord = words[1]\n",
    "    thirdWord = words[2]\n",
    "    count = count\n",
    "    cProb = count * 1.0 / unigramsCounter[firstWord]\n",
    "    conditionalProbabilityDict[firstWord+\" \"+secondWord + \" \" +thirdWord] = cProb\n",
    "\n",
    "# print conditionalProbabilityDict\n",
    "file = open(conditionalProbabilityFile,\"wb\")\n",
    "pickle.dump(conditionalProbabilityDict,file)\n",
    "\n",
    "file = open(trigramsCounterPath,\"wb\")\n",
    "pickle.dump(trigramsCounter,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a word to predict its next probable words (':)' for stopping) : can i t\n",
    "\n",
    "Bigrams prediction:\n",
    "\n",
    "i think : 0.08613532912264124\n",
    "\n",
    "i thought : 0.014199842979509856\n",
    "\n",
    "i told : 0.006020473347966184\n",
    "\n",
    "____________________\n",
    "\n",
    "Trigrams prediction:\n",
    "\n",
    "can i tell : 0.0009584862682746684\n",
    "\n",
    "can i take : 0.00035332434987379933\n",
    "\n",
    "can i talk : 0.00034392742567502805-\n",
    "\n",
    "--------------\n",
    "\n",
    "Enter a word to predict its next probable words (':)' for stopping) : should we go to\n",
    "\n",
    "Bigrams prediction:\n",
    "\n",
    "to the : 0.10137702785084556\n",
    "\n",
    "to be : 0.06316283346651083\n",
    "\n",
    "to a : 0.02362025832529113\n",
    "\n",
    "____________________\n",
    "\n",
    "Trigrams prediction:\n",
    "\n",
    "go to the : 0.08628557038529337\n",
    "\n",
    "go to a : 0.023319063208259883\n",
    "\n",
    "go to school : 0.010480987156887434\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed**\n",
    "\n",
    "Mostly, it can give the result in 1500 ms. So, it is somewhat slow procedure. However, we can use different methods to store and call the words. But, I do not know so much things about data structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**With More Time**\n",
    "\n",
    "- Implement the SQLITE type storage to get results faster. http://zetcode.com/db/sqlitepythontutorial/\n",
    "\n",
    "- Calculate the perplexity for test set.\n",
    "\n",
    "- Implement the linear interpolation to guess next word.\n",
    "\n",
    "- Try to implement with low-level language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyannote]",
   "language": "python",
   "name": "conda-env-pyannote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
